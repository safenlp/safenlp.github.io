[
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Stay updated by subscribing to our Substack or following us on GitHub."
  },
  {
    "objectID": "publications/index.html#coming-soon",
    "href": "publications/index.html#coming-soon",
    "title": "Publications",
    "section": "",
    "text": "Stay updated by subscribing to our Substack or following us on GitHub."
  },
  {
    "objectID": "events/index.html",
    "href": "events/index.html",
    "title": "Key Talks & Events",
    "section": "",
    "text": "Understanding Language Models, Their Constraints and Trust Factors\n\n\nKey talk at Manisa Celal Bayar University\n\n\n\n\n\nApr 2025\n\n\n\n\n\n\n\nLanguage Models: Capabilities, Limitations, and Trust\n\n\nKey talk at Hali√ß University\n\n\n\n\n\nJan 2025\n\n\n\n\n\n\n\nTrustworthiness and Weaknesses of LLMs\n\n\nKey talk at Marmara University\n\n\n\n\n\nDec 2024\n\n\n\n\n\n\n\nSafeNLP & Evaluation of LLMs\n\n\nKey talk at I≈üƒ±k University | IEEEXtreme\n\n\n\n\n\nJul 2024\n\n\n\n\n\n\n\nTrustworthiness and Weaknesses of LLMs\n\n\nKey talk at Istanbul University | 100. Yƒ±l Bili≈üim √áalƒ±≈ütayƒ±\n\n\n\n\n\nMar 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "events/2025-01-halic/index.html",
    "href": "events/2025-01-halic/index.html",
    "title": "Language Models: Capabilities, Limitations, and Trust",
    "section": "",
    "text": "Hali√ß University January 2025\nLanguage Models: Capabilities, Limitations, and Trust"
  },
  {
    "objectID": "events/2024-07-isik/index.html",
    "href": "events/2024-07-isik/index.html",
    "title": "SafeNLP & Evaluation of LLMs",
    "section": "",
    "text": "I≈üƒ±k University | IEEEXtreme July 2024\nSafeNLP & Evaluation of LLMs"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact SafeNLP",
    "section": "",
    "text": "Our mission is to harness the potential of AI and NLP technologies while mitigating risks and ensuring ethical use. We are committed to building a safer and more reliable future for artificial intelligence and natural language processing.\nExplore our resources and join our community today."
  },
  {
    "objectID": "contact.html#join-our-mission",
    "href": "contact.html#join-our-mission",
    "title": "Contact SafeNLP",
    "section": "",
    "text": "Our mission is to harness the potential of AI and NLP technologies while mitigating risks and ensuring ethical use. We are committed to building a safer and more reliable future for artificial intelligence and natural language processing.\nExplore our resources and join our community today."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "üìñ Subscribe on Substack \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVulnerable AI + Unaware Users + High Stakes = Crisis\n\n\nThe Critical Landscape of LLMs Adoption - examining the perfect storm of vulnerable AI systems, inexperienced users, and high-stakes applications\n\n\n\n\n\nJun 2025\n\n\nMehmet Ali √ñzer\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/1-vulnerable-ai-crisis/index.html",
    "href": "blog/1-vulnerable-ai-crisis/index.html",
    "title": "Vulnerable AI + Unaware Users + High Stakes = Crisis",
    "section": "",
    "text": "üìñ Also available on Substack: Read this post on Substack"
  },
  {
    "objectID": "blog/1-vulnerable-ai-crisis/index.html#the-critical-landscape-of-llms-adoption",
    "href": "blog/1-vulnerable-ai-crisis/index.html#the-critical-landscape-of-llms-adoption",
    "title": "Vulnerable AI + Unaware Users + High Stakes = Crisis",
    "section": "The Critical Landscape of LLMs Adoption",
    "text": "The Critical Landscape of LLMs Adoption\nWe‚Äôre living through an AI deployment experiment at global scale‚Äîand the results are alarming. Large Language Models started as general chatbots, then quickly spread to education platforms, financial services, and even healthcare systems. What began as simple conversational tools has evolved into AI making decisions about loan approvals, medical diagnoses, and legal advice‚Äîoften deployed by developers who don‚Äôt fully understand the risks they‚Äôre introducing. These vulnerable AI systems carry hidden flaws and unpredictable behaviors that even their creators struggle to control. Meanwhile, security researchers discover new vulnerabilities faster than patches can be developed, creating an ever-widening security gap.\nAt the same time, inexperienced users‚Äîfrom students to executives‚Äîare making critical decisions based on AI outputs they‚Äôre not equipped to evaluate. They trust AI recommendations for medical advice, financial planning, and business strategy without understanding the limitations or potential for manipulation.\nThese AI systems now handle high-stakes applications that affect real lives, real money, and real safety. Healthcare diagnoses, legal advice, educational assessments, and security decisions increasingly rely on technology that remains fundamentally unpredictable.\nThis convergence is creating a perfect storm:\n\nVulnerable AI + Unaware Users + High Stakes = Crisis"
  },
  {
    "objectID": "blog/1-vulnerable-ai-crisis/index.html#two-sides-of-the-coin-safety-and-security",
    "href": "blog/1-vulnerable-ai-crisis/index.html#two-sides-of-the-coin-safety-and-security",
    "title": "Vulnerable AI + Unaware Users + High Stakes = Crisis",
    "section": "Two Sides of the Coin: Safety and Security",
    "text": "Two Sides of the Coin: Safety and Security\nThis crisis has two faces: safety risks where LLMs cause harm simply by doing what they‚Äôre designed to do‚Äîgenerating biased content, spreading misinformation, or giving dangerous advice‚Äîand security risks where attackers exploit LLM vulnerabilities to steal data, manipulate outputs, or weaponize these systems against users.\nThe danger is that we‚Äôre racing to deploy these AI systems faster than we can secure them. This is the reality of LLM security and safety in 2025.\n\nFrom the User‚Äôs Perspective: LLM Safety is Paramount\nStudents researching for assignments, patients seeking health information, and everyday users making decisions based on AI recommendations need assurance that these systems won‚Äôt:\n\nMislead them with misinformation\nDiscriminate against them through biased outputs\nManipulate their opinions\nProvide dangerous advice that could harm their health, finances, or well-being\n\nSociety demands AI systems that respect privacy, avoid generating harmful content, and don‚Äôt perpetuate discrimination or spread false information that could destabilize communities or democratic processes.\n\n\nFrom the Business and Technical Perspective: LLM Security is Equally Critical\nDevelopers integrating AI into applications, business owners deploying customer-facing chatbots, executives making strategic AI investments, and stakeholders responsible for organizational risk all need confidence that these systems can‚Äôt be weaponized against them. They require assurance that attackers won‚Äôt:\n\nExploit prompt injection vulnerabilities to steal sensitive data\nManipulate AI outputs to damage reputation\nExtract proprietary training information\nTurn their own AI systems into tools for cyber-attacks against their customers and partners\n\nBoth sides of this coin are essential‚Äîusers need safe AI that serves their best interests, while organizations need secure AI that can‚Äôt be misused for malicious purposes. Unfortunately, current LLM deployment often fails on both fronts."
  },
  {
    "objectID": "blog/1-vulnerable-ai-crisis/index.html#playing-with-fire-at-scale",
    "href": "blog/1-vulnerable-ai-crisis/index.html#playing-with-fire-at-scale",
    "title": "Vulnerable AI + Unaware Users + High Stakes = Crisis",
    "section": "Playing with Fire at Scale",
    "text": "Playing with Fire at Scale\n\nLLM Safety Failures: Real-World Harm\nAir Canada‚Äôs Chatbot Misinformation (February 2024) Air Canada‚Äôs chatbot provided incorrect bereavement policy information, leading to a court ruling that ordered the airline to pay CA$650.88 in damages after a customer relied on false information about post-travel discount eligibility.\nGoogle‚Äôs AI Overviews Dangerous Advice (2024) Google‚Äôs AI Overviews feature, reaching over 1 billion users by end of 2024, generated dangerous advice including adding ‚Äú1/8 cup of non-toxic glue‚Äù to pizza sauce and recommending adding oil to cooking fires to ‚Äúhelp put it out.‚Äù\nNYC‚Äôs MyCity Chatbot Illegal Guidance (October 2023) New York City‚Äôs MyCity chatbot, launched in October 2023, encouraged illegal business practices by falsely claiming employers could take workers‚Äô tips and fire employees for sexual harassment complaints.\nDoNotPay‚Äôs AI Lawyer Fraud ($193,000 FTC Fine, September 2024) The FTC imposed a $193,000 fine on DoNotPay for marketing ‚Äúsubstandard and poorly done‚Äù legal documents from its ‚ÄúAI lawyer‚Äù service between 2021-2023, affecting thousands of subscribers who received inadequate legal advice.\n\n\nLLM Security Breaches: Systematic Vulnerabilities\nOpenAI ChatGPT Data Breach (March 2023) A Redis library vulnerability exposed personal data from approximately 101,000 ChatGPT users, including conversation titles, names, email addresses, and partial credit card numbers. A separate OpenAI breach in early 2023, reported by the New York Times in July 2024, saw hackers gain access to internal employee discussion forums about AI technology development.\nMicrosoft Copilot Zero-Click Attack (2025) Microsoft‚Äôs Copilot faced a critical vulnerability that enabled zero-click attacks through malicious emails, allowing attackers to automatically search and exfiltrate sensitive data from Microsoft 365 environments.\nLLM Hijacking Surge (July 2024) Sysdig research documented a 10x increase in LLM hijacking attacks during July 2024, with stolen cloud credentials used to rack up $46,000-$100,000+ per day in unauthorized AI service usage costs across platforms including Claude, OpenAI, and AWS Bedrock.\nMass Credential Theft (2024) Security firm KELA identified over 3 million compromised OpenAI accounts collected in 2024 alone through infostealer malware, with credentials actively sold on dark web marketplaces."
  },
  {
    "objectID": "blog/1-vulnerable-ai-crisis/index.html#bridging-the-ai-safety-gap-safenlps-accessibility-mission",
    "href": "blog/1-vulnerable-ai-crisis/index.html#bridging-the-ai-safety-gap-safenlps-accessibility-mission",
    "title": "Vulnerable AI + Unaware Users + High Stakes = Crisis",
    "section": "Bridging the AI Safety Gap: SafeNLP‚Äôs Accessibility Mission",
    "text": "Bridging the AI Safety Gap: SafeNLP‚Äôs Accessibility Mission\nThe current AI safety landscape presents a critical disconnect: while academic research produces sophisticated security frameworks and industry develops advanced technical solutions, these innovations remain largely inaccessible to the broader community that needs them most. Complex research papers, technical documentation, and enterprise-grade tools create barriers that prevent everyday users, small organizations, and non-technical decision-makers from effectively participating in AI safety practices.\nSafeNLP addresses this accessibility gap by serving as a translator between academic rigor and practical usability. Our mission recognizes that sustainable AI progress requires informed decision-making at every level:\n\nIndividual users integrating AI into their workflows need simple guidelines and red flags to recognize\nApplication developers building LLM-powered products require practical testing tools and implementation frameworks\nExecutives making strategic AI adoption decisions need risk assessment matrices and compliance roadmaps\n\nThe sophisticated safety ecosystem currently demands specialized expertise that most organizations lack, creating an environment where only well-resourced entities can meaningfully participate in AI safety. SafeNLP‚Äôs mission challenges this exclusivity by democratizing access to safety knowledge through:\n\nIntuitive interfaces\nPractical toolkits\nEducational resources that speak to different technical literacy levels\n\nWe transform: - Academic insights ‚Üí Actionable guidance - Complex security frameworks ‚Üí User-friendly checklists - Theoretical vulnerabilities ‚Üí Testable scenarios\n\nOur Philosophy\nThe philosophy underlying this ecosystem emphasizes that AI safety is not a zero-sum competition but a shared endeavor that benefits from open collaboration, diverse perspectives, and inclusive participation. This principle directly informs SafeNLP‚Äôs approach to making security knowledge accessible across different communities and expertise levels.\n\nMehmet Ali √ñzer maliozer@safenlp.org"
  },
  {
    "objectID": "blog/1-vulnerable-ai-crisis/index.html#references",
    "href": "blog/1-vulnerable-ai-crisis/index.html#references",
    "title": "Vulnerable AI + Unaware Users + High Stakes = Crisis",
    "section": "References",
    "text": "References\n\nOWASP Foundation. (2025). OWASP Top 10 for LLM Applications & Generative AI: Key Updates for 2025. 2025 Security Updates: OWASP Top 10 for LLMs & GenAI\nLasso Security. (2025). LLM Security Predictions: What‚Äôs Ahead in 2025. LLM Security Predictions\nPrompt Security. (2024). 8 Real World Incidents Related to AI. https://www.prompt.security/blog/8-real-world-incidents-related-to-ai\nMIT Technology Review. (2024). The biggest AI flops of 2024. https://www.technologyreview.com/2024/12/31/1109612/biggest-worst-ai-artificial-intelligence-flops-fails-2024/\nFederal Trade Commission. (2024). DoNotPay. https://www.ftc.gov/legal-library/browse/cases-proceedings/donotpay\nTwingate. (2024). What happened in the ChatGPT data breach? https://www.twingate.com/blog/tips/chatgpt-data-breach\nReuters. (2024). OpenAI‚Äôs internal AI details stolen in 2023 breach, NYT reports. https://www.reuters.com/technology/cybersecurity/openais-internal-ai-details-stolen-2023-breach-nyt-reports-2024-07-05/\nFortune. (2025). Microsoft Copilot zero-click attack raises alarms about AI agent security. https://fortune.com/2025/06/11/microsoft-copilot-vulnerability-ai-agents-echoleak-hacking/\nAdversa AI. (2024). LLM Security TOP Digest: From Incidents and Attacks to Platforms and Protections. https://adversa.ai/blog/llm-security-top-digest-from-incidents-and-attacks-to-platforms-and-protections/\nThe Hacker News. (2024). Over 225,000 Compromised ChatGPT Credentials Up for Sale on Dark Web Markets. https://thehackernews.com/2024/03/over-225000-compromised-chatgpt.html"
  },
  {
    "objectID": "codes/index.html",
    "href": "codes/index.html",
    "title": "SafeNLP Codes",
    "section": "",
    "text": "We are developing open-source tools and resources for AI safety and security. Our upcoming releases will include:\n\nLLM Security Testing Frameworks - Practical tools for identifying vulnerabilities in LLM applications\nSafety Assessment Toolkits - Resources for evaluating AI system safety and reliability\nEducational Resources & Blog - Code examples and tutorials for secure AI development\n\nStay tuned for updates. Follow our GitHub organization for the latest releases.\nSubscribe to our Substack to get notified when new tools are available."
  },
  {
    "objectID": "codes/index.html#coming-soon",
    "href": "codes/index.html#coming-soon",
    "title": "SafeNLP Codes",
    "section": "",
    "text": "We are developing open-source tools and resources for AI safety and security. Our upcoming releases will include:\n\nLLM Security Testing Frameworks - Practical tools for identifying vulnerabilities in LLM applications\nSafety Assessment Toolkits - Resources for evaluating AI system safety and reliability\nEducational Resources & Blog - Code examples and tutorials for secure AI development\n\nStay tuned for updates. Follow our GitHub organization for the latest releases.\nSubscribe to our Substack to get notified when new tools are available."
  },
  {
    "objectID": "events/2024-03-istanbul/index.html",
    "href": "events/2024-03-istanbul/index.html",
    "title": "Trustworthiness and Weaknesses of LLMs",
    "section": "",
    "text": "Istanbul University | 100. Yƒ±l Bili≈üim √áalƒ±≈ütayƒ± March 2024\nTrustworthiness and Weaknesses of LLMs"
  },
  {
    "objectID": "events/2024-12-marmara/index.html",
    "href": "events/2024-12-marmara/index.html",
    "title": "Trustworthiness and Weaknesses of LLMs",
    "section": "",
    "text": "Marmara University December 2024\nTrustworthiness and Weaknesses of LLMs"
  },
  {
    "objectID": "events/2025-04-mcbu/index.html",
    "href": "events/2025-04-mcbu/index.html",
    "title": "Understanding Language Models, Their Constraints and Trust Factors",
    "section": "",
    "text": "Manisa Celal Bayar University April 2025\nUnderstanding Language Models, Their Constraints and Trust Factors"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "safenlp.org",
    "section": "",
    "text": "SafeNLP: Ensuring Secure and Ethical Language Models\n\n\nSafety solutions and research for NLP, LLM, and AI\n\n\n\n\n\n\n\nWe explore safety solutions and research for NLP, LLM, and AI, designing approaches for both academic and industry levels with emphasis on security and ethical principles. Our work spans bias detection and mitigation, privacy-preserving techniques, adversarial robustness, content moderation, transparency, and comprehensive safety benchmarking for language models.\nGitHub | LinkedIn"
  }
]